{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertoorazi/Repositories/Facultad/tuia/cuarto-cuatrimestre/aa1/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, epochs=50, batch_size=16, learning_rate=0.01):\n",
    "        #inicializo algunos parámetros como épocas, batch_size, learning rate\n",
    "        #(no son necesarios)\n",
    "        #se puede agregar la cantidad de capas, la cantidad de neuronas por capa (pensando en hacer una clase que pueda ser usada para cualquier caso)\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self, input_shape, num_classes):\n",
    "        # ejemplo con 2 capas ocultas de 64 neuronas y activación softmax (multiclase, recibe la cantidad de clases como input, además del input_shape)\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        #compilo el modelo con el optimizador Adam, la función de pérdida categorical_crossentropy y la métrica accuracy\n",
    "        #totalmente optimizable e incluso pueden ser parámetros de la función build_model\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    def train(self, X_train, y_train, X_valid, y_valid):\n",
    "        # simplemente el fit del modelo. Devuelvo la evolución de la función de pérdida, ya que es interesante ver como varía a medida que aumentan las épocas!\n",
    "        history=self.model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=self.epochs, batch_size=self.batch_size)\n",
    "        return history.history['loss']\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        ### evalúo en test\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test)\n",
    "        print(f\"test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        ### predicciones\n",
    "        predictions = self.model.predict(X_new)\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    ### acá también me podría armar una función para graficar la evolución de la función de pérdida en train y validación, etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "nn = NeuralNetwork(epochs=50, batch_size=16, learning_rate=0.01)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "### en este caso tenemos más de 2 clases, por lo que necesitamos hacer one hot encoding en la salida, para este modelo en particular.\n",
    "y_train=tf.keras.utils.to_categorical(y_train)\n",
    "y_valid=tf.keras.utils.to_categorical(y_valid)\n",
    "y_test=tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "#buildeo el modelo\n",
    "nn.build_model(input_shape=X_train.shape[1], num_classes=y_train.shape[1])\n",
    "\n",
    "# entreno el modelo\n",
    "history=nn.train(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# evaluo metricas\n",
    "nn.evaluate(X_test, y_test)\n",
    "\n",
    "## predicciones..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)\n",
    "plt.title('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend('train', 'validation', )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkRegressor:\n",
    "    def __init__(self, epochs=50, batch_size=16, learning_rate=0.01):\n",
    "        #inicializo algunos parámetros como épocas, batch_size, learning rate\n",
    "        #(no son necesarios)\n",
    "        #se puede agregar la cantidad de capas, la cantidad de neuronas por capa\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        # ejemplo con 2 capas ocultas de 64 neuronas, la saida en este caso es solo una neurona, con activación lineal\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=tf.keras.regularizers.l2(0.01)),# El valor es el alpha\n",
    "            tf.keras.layers.Dropout(self.dropout_rate),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(self.dropout_rate),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        #compilo el modelo con el optimizador Adam, la función de pérdida mse y métrica mse también\n",
    "        #totalmente optimizable e incluso pueden ser parámetros de la función build_model\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, X_train, y_train, X_valid, y_valid):\n",
    "        # simplemente el fit del modelo. Devuelvo la evolución de la función de pérdida, ya que es interesante ver como varía a medida que aumentan las épocas!\n",
    "        history=self.model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=self.epochs, batch_size=self.batch_size)\n",
    "        return history.history['loss'], history.history['val_loss']\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        ### evalúo en test\n",
    "        loss, mse = self.model.evaluate(X_test, y_test)\n",
    "        print(f\"test MSE: {mse:.4f}\")\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        ### predicciones\n",
    "        predictions = self.model.predict(X_new)\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    ### acá también me podría armar una función para graficar la evolución de la función de pérdida en train y validación, etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_train = x_scaler.fit_transform(X_train)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "X_valid = x_scaler.transform(X_valid)\n",
    "y_valid = y_scaler.transform(y_valid)\n",
    "X_test = x_scaler.transform(X_test)\n",
    "y_test = y_scaler.transform(y_test)\n",
    "\n",
    "nnr= NeuralNetworkRegressor(epochs=50, batch_size=16, learning_rate=0.01)\n",
    "nnr.build_model(input_shape=X_train.shape[1], num_classes=y_train.shape[1])\n",
    "history=nnr.train(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# evaluo metricas\n",
    "nnr.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
